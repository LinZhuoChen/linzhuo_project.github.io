---
title: Deep Interest Network (DIN) 介绍
categories: graph embedding
tags:
- 推荐系统
---

大家好，这里是Linzhuo，今天为大家介绍一个推荐系统中的经典模型：Deep Interest Network (DIN)。该模型首次在推荐系统中引入了注意力机制，来对用户的点击历史进行加权。

DIN的核心思想是：对于当前要打分的item，并不是所有的点击历史都同样重要。例如判断用户是否想购买篮球，那么用户点击历史中的球鞋，哑铃就会对购买篮球起到关键作用。而对于用户之前点击的其他物品，例如笔记本等，则不应该与是否购买篮球建立起联系。DIN模型通过注意力机制，来为用户的点击历史进行加权，对与候选物品相关历史提升权重，同时降低无关历史的权重。我们接下来将详细介绍DIN模型的技术细节。

### 基础模型

图1为DIN的基础模型结构，用户特征（id），候选物品的特征（id），用户的点击历史特征（id），均通过embeeding层获得其embedding向量，之后通过sum pooling 和全连接层得到最后候选物品的点击概率。其中embedding是一种将高维稀疏（binary）向量转化为低维稠密语义向量的方法。我们令 $W^i = [W_1^i, W_2^i, ..., W_j^i,...,W_{K_i}^i] \in R^{D\times K_i}$ 为第 $i$ 个embedding 权重，其中 $W^i_j \in R^D$。embedding层的流程如下：

- 如果 $t_i$ 为一个one-hot向量，其中 $t_i[j] = 1$，则 $t_i$ 通过embedding 层的结果为 $e_i = W^i_j$。
- 如果 $t_i$ 为multi-hot 向量，且 $t_i[j] = 1 \ for \  j \in \{i_1, i_2,...,i_k\}$。则该multi-hot向量的embedding表示为 $\{e_{i_1}, e_{i_2},...,e_{i_k}\} = \{W^i_{i_1}, W^i_{i_2},...,W^i_{i_k}\}$.
![](https://files.mdnice.com/user/16260/0b1776a6-0b2b-4184-ac32-c8051cdc0285.png)
<center>图1:基础模型结构</center>

对于基础模型来说，损失函数的定义如下：


$$
L = -\frac{1}{N} \sum_{(x,y) \in S} (ylog(p(x) + (1-y)log(1-p(x))))
$$


其中 $x$ 为模型的输入，$p(x)$ 为模型预测用户对候选物品的点击概率，$S$ 为大小为 $N$ 的训练集合。

### DIN模型结构
相较于图1中的基础模型，图2中的DIN模型通过Activation Unit来为用户的点击历史分配不同的注意力权重。给定候选物品 $A$，以及用户 $U$ 的点击历史embedding $\{e_1, e_2,...,e_H\}$，我们给点击历史分配不同的权重并求和，得到整体的embedding表示，具体流程如下：


$$
v_U(A) = f(v_A, e_1, e_2, ..., e_H) = \sum_{j=1}^H \alpha(e_j,v_A)e_j = \sum_{j=1}^H w_je_j.
$$


![](https://files.mdnice.com/user/16260/bfff414c-a49b-4b6f-a26f-ab35e186c440.png)
<center>图2:DIN模型结构</center>

其中 $v_A$ 为候选物品 $A$ 的embedding表示。$\alpha(.)$ 为前向全连接网络，输出则为注意力权重。细节如图2右上角所示。
我们以开源库[DeepCTR](https://github.com/shenweichen/DeepCTR "DeepCTR")中DIN的核心代码为例，来解释DIN模型对点击历史的处理，部分流程的说明在注释中给出。

```python
def LocalActivationUnit(inputs, training=None, **kwargs):
    # keys 为点击历史
    query, keys = inputs

    keys_len = keys.get_shape()[1]
    queries = K.repeat_elements(query, keys_len, 1)
        
    # 通过DNN，输入候选item和点击历史，计算每个点击历史的注意力权重
    att_input = tf.concat(
        [queries, keys, queries - keys, queries * keys], axis=-1)
    att_out = self.dnn(att_input, training=training)
    attention_score = self.dense([att_out, self.kernel, self.bias])

    return attention_score
```

上述代码即为图2右上角的流程，在得到注意力权重后，我们即可通过点击历史得到用户兴趣的embedding表示：

```python
attention_score = LocalActivationUnit([queries, keys], training=training)
outputs = tf.transpose(attention_score, (0, 2, 1))
outputs = tf.matmul(outputs, keys)
```

### 总结
DIN模型通过建立起候选物品与点击历史之间的注意力系数，来为不同的点击历史进行加权，相较于图1中的基础模型，DIN得到了更丰富的用户兴趣表示。这也为之后推荐模型的用户兴趣表达研究奠定了基础。




