---
title: MaskNet网络介绍
categories: graph embedding
tags:
- NLP
---

FNN, DeepFM, XDeepFM是我们推荐算法领域中较为知名的方法，这些方法使用浅层网络来对特征进行交叉。但这些模型真的充分提取了输入特征了吗？《MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask》这篇文章提出了MaskNet， 利用instance-guided mask，提取全局上下文信息，并动态地通过逐点相乘的方法融入到特征嵌入层和前馈层，来突出重要的特征。 这篇论文的主要贡献如下：

- 提出了利用instance-guided mask，通过逐点相乘的方法融入到特征嵌入层和前馈层来动态的调整特征的权重。instance-guided mask包含了特征的全局信息。
- 提出了一种以instance-guided mask 为基础的MaskBlock模块，由三个要素组成：instance-guided mask，全连接网络和layer normalization。
- 以MaskBlock模块为基础，提出了一种新颖的排序模型：MaskNet。


## Feature Embedding Layer
首先，我们用$e_i \in R^k$ 表示输入特征的embedding向量，该向量通过embedding matrix查表(类别特征)或者dnn映射(数值特征)得到。Embedding Layer的表示如下所示：


$$
V_{emb} = concate(e_1, e_2, ..., e_i, ..., e_f).
$$


其中$f$为特征域的数目。$V_{emb} \in R^{f\times k}$。

## Instance-Guided Mask

得到$V_{emb}$后，我们利用$V_{emb}$产生Instance-Guided Mask，来动态地调整网络中特征的权重。公式如下：


$$
V_{mask} = W_{d2}(W_{d1}V_{emb} + \beta_{d1}) + \beta_{d2}.
$$


其中$V_{emb}$为Feature Embedding Layer，$W_{d1}, W_{d2}$为下图中两个全连接层(Aggregation Layer, Projection Layer)的权重：

![](https://files.mdnice.com/user/5787/21804985-f60e-443d-aa32-0ba6e5c2038a.png)

$V_{mask}$，即为Instance-Guided Mask，由$V_{emb}$产生，包含输入特征的全局信息，$V_{mask}$可以通过逐点相乘的方法，在MaskBlock模块中，动态地调整特征的权重。

## MaskBlock 和 MaskNet
MaskBlock模块的结构如下图所示：


![](https://files.mdnice.com/user/5787/a258c61c-9c19-4009-b374-e815555cf496.png)

其流程可以用公式表示为：


$$
V_{maskedEMB} = V_{mask} \odot LN\_EMB(V_{emb}) 
$$



$$
V_{output} = ReLU(LN(W_i V_{maskedEMB}))
$$



其中：


$$
LN\_EMB(V_{emb})= concate(LN(e_1), LN(e_2), ..., LN(e_i), ..., LN(e_f))
$$


LN为Layer Normalization操作，$\odot$为逐点相乘。

基于MaskBlock模块，论文给出了两种方案，通过串行或者并行的方法，堆叠MaskBlock模块，得到MaskNet，网络结构如下图所示：


![](https://files.mdnice.com/user/5787/1f039d51-4f0c-4d18-a8f4-e7baff475da1.png)

最后的prediction layer和之前的CTR模型一样。使用交叉熵损失函数优化即可。

## 总结
其实这篇文章与推荐领域的FIBINET，视觉领域的SENet和语音识别领域的LHUC有很多相似之处，核心思想都是通过自注意力机制加强网络的特征提取能力。大家不妨可以在自己的任务上试一试，欢迎交流～